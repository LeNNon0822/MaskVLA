import{c as e,a,b as t,d as n,F as o,o as s,e as i}from"./@vue-DNEHza7E.js";!function(){const e=document.createElement("link").relList;if(!(e&&e.supports&&e.supports("modulepreload"))){for(const e of document.querySelectorAll('link[rel="modulepreload"]'))a(e);new MutationObserver(e=>{for(const t of e)if("childList"===t.type)for(const e of t.addedNodes)"LINK"===e.tagName&&"modulepreload"===e.rel&&a(e)}).observe(document,{childList:!0,subtree:!0})}function a(e){if(e.ep)return;e.ep=!0;const a=function(e){const a={};return e.integrity&&(a.integrity=e.integrity),e.referrerPolicy&&(a.referrerPolicy=e.referrerPolicy),"use-credentials"===e.crossOrigin?a.credentials="include":"anonymous"===e.crossOrigin?a.credentials="omit":a.credentials="same-origin",a}(e);fetch(e.href,a)}}();const r="/MaskVLA/static/mp4/video1-CG06VzUN.mp4";i(((e,a)=>{const t=e.__vccOpts||e;for(const[n,o]of a)t[n]=o;return t})({},[["render",function(i,c){return s(),e(o,null,[c[0]||(c[0]=a("div",{class:"articleTitleContainer contentContainer"},[a("div",{class:"articleTitle"},"MaskVLA:"),a("div",{class:"articleTitle"}," Visual Masking Against Trajectory Overfitting of Vision-Language-Action Model "),a("div",{class:"arthorNameLine"},[a("div",{class:"arthorNameContainer"},[a("span",{class:"arthorName"},"anonymous author 1"),a("span",{class:"sup"},"1"),a("span",{class:"arthorName"},"， ")]),a("div",{class:"arthorNameContainer"},[a("span",{class:"arthorName"},"anonymous author 2"),a("span",{class:"sup"},"1"),a("span",{class:"arthorName"},"， ")]),a("div",{class:"arthorNameContainer"},[a("span",{class:"arthorName"},"anonymous author N"),a("span",{class:"sup"},"N"),a("span",{class:"arthorName"},"， ")])]),a("div",{class:"expression"},[a("div",{class:"expressionItem"},[a("span",{class:"sup"}," * "),a("span",{class:"expressionText"},"Equal Contribution")])]),a("div",{class:"university"},[a("div",{class:"universityContainer"},[a("span",{class:"sup"}," 1 "),a("span",{class:"universityName"},"anonymous organization ")])]),a("div",{class:"buttonGroup"},[a("button",{class:"button",onclick:"window.open('/')",type:"button"},[a("div",{class:"buttonIcon arxivIcon"}),n(" Paper ")]),a("button",{class:"button",onclick:"window.open('/')",type:"button"},[a("div",{class:"buttonIcon gitHubIcon"}),n(" Code ")]),a("button",{class:"button",onclick:"window.open('/')",type:"button"},[a("div",{class:"buttonIcon huggingFaceIcon"}),n(" Model ")])])],-1)),c[1]||(c[1]=a("div",{class:"videoDisplayContainer contentContainer coverVideo"},[a("video",{muted:"",playsinline:"",loop:"",controls:"","disable-picture-in-picture":"true"},[a("source",{src:r,type:"video/mp4"})])],-1)),c[2]||(c[2]=t('<div class="articleMainBodyContainer contentContainer" data-v-5ff7179a><div class="contentMainTitle" data-v-5ff7179a>Abstract</div><div class="content" data-v-5ff7179a> Vision-Language-Action (VLA) models integrate vision-language understanding with executable robot actions, enabling end-to-end learning for robot control. However, our empirical analysis reveals that existing models exhibit severe trajectory memorization and overfitting when finetuned on limited datasets. To guide the model in effectively utilizing wrist camera information, we propose MaskVLA, a masking-based fine-tuning strategy. By randomly masking a small portion of the main camera’s visual information, the model is guided to autonomously learn more fine-grained, task-relevant, and effective visual features. This process leads to the emergence of robust policies, thereby enhancing the model’s capability to tackle complex manipulation tasks and improving its generalization performance. Our method has been comprehensively evaluated on RoboTwin 2.0, achieving an average success rate improvement of 22.9% and 16.5% compared to π0 and OpenVLA-OFT, respectively. Furthermore, experiments on real-world ALOHA robots also demonstrate the effectiveness of our approach. Our project page is https://anonymous.4open.science/w/ MaskVLA-D31F/. </div></div><div class="articleMainBodyContainer contentContainer" data-v-5ff7179a><div class="contentImg contentImg1" data-v-5ff7179a><img src="/MaskVLA/static/png/pic1-C4px2jGP.png" alt="image" data-v-5ff7179a><div class="introText" data-v-5ff7179a><span class="bold" data-v-5ff7179a>Fig. 1.</span> We propose MaskVLA, an improved fine-tuning method for multiview VLA frameworks that significantly enhances model performance. By randomly masking the primary camera input, the method guides the model to learn robust multi-view visual features. In both RoboTwin2.0 [5] simulation environments and real-world experiments, it demonstrates substantial improvements over OpenVLA-OFT [2] across multiple manipulation tasks. </div></div></div>',2)),c[3]||(c[3]=a("div",{class:"videoDisplayGroupContainer contentContainer"},[a("div",{class:"coverVideo"},[a("video",{muted:"",playsinline:"",loop:"",controls:"","disable-picture-in-picture":"true"},[a("source",{src:r,type:"video/mp4"})])]),a("div",{class:"coverVideo"},[a("video",{muted:"",playsinline:"",loop:"",controls:"","disable-picture-in-picture":"true"},[a("source",{src:"/MaskVLA/static/mp4/video2-7ozfC2UR.mp4",type:"video/mp4"})])])],-1))],64)}],["__scopeId","data-v-5ff7179a"]])).mount("#app");
